import argparse
import json
import os
from datetime import datetime
from typing import Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc
from sklearn.model_selection import train_test_split

# Supervised training with optional weak labels.
# - Input: features parquet (with columns = features [+ optional label])
# - Optional: weak labels parquet generated by train_pyod.py (column label_weak)
# - Output: model_dir/trained_fraud_model.json compatible with Go loader
#   Fields: Version, Weights (includes _bias), Threshold, Features, UpdatedAt, Metrics
#   Threshold is picked to reach desired precision/recall tradeoff (defaults).


def _load_datasets(features_path: str, weak_labels_path: Optional[str]) -> Tuple[pd.DataFrame, Optional[pd.Series]]:
    df = pd.read_parquet(features_path)
    y_true = None
    if 'label' in df.columns:
        y_true = df['label'].astype(int)
        df = df.drop(columns=['label'])

    if weak_labels_path and os.path.exists(weak_labels_path):
        w = pd.read_parquet(weak_labels_path)
        # Align by index if possible, else by row order
        if 'label_weak' in w.columns:
            y_weak = w['label_weak'].astype(int)
        else:
            raise ValueError("weak labels parquet must include 'label_weak' column")
    else:
        y_weak = None

    return df, y_true if y_true is not None else y_weak


def _train_logreg(X: np.ndarray, y: np.ndarray, C: float = 1.0, max_iter: int = 1000) -> Tuple[CalibratedClassifierCV, np.ndarray]:
    base = LogisticRegression(
        solver='liblinear',
        max_iter=max_iter,
        n_jobs=None,
        class_weight='balanced',
        C=C
    )
    # Platt scaling with 3-fold CV for robust probability calibration
    clf = CalibratedClassifierCV(base, method='sigmoid', cv=3)
    clf.fit(X, y)
    # Extract linear coefficients from the underlying estimator(s)
    # CalibratedClassifierCV has calibrated_classifiers_ list; each has base_estimator
    # We'll average coefficients across folds for export.
    coefs = []
    intercepts = []
    for cc in clf.calibrated_classifiers_:
        est = cc.base_estimator
        # Binary: coef_ shape (1, n_features)
        coefs.append(est.coef_.ravel())
        intercepts.append(float(est.intercept_.ravel()[0]))
    coef_mean = np.mean(np.stack(coefs, axis=0), axis=0)
    intercept_mean = float(np.mean(intercepts))
    return clf, np.concatenate([coef_mean, [intercept_mean]])


def _choose_threshold(y_true: np.ndarray, scores: np.ndarray, target_precision: float, min_recall: float) -> float:
    # Use precision-recall curve to pick smallest threshold achieving >= target_precision while maintaining >= min_recall
    precision, recall, thresholds = precision_recall_curve(y_true, scores)
    # thresholds array is len = n_points-1; precision/recall align with points
    chosen = 0.5
    best = None
    for p, r, t in zip(precision[:-1], recall[:-1], thresholds):
        if p >= target_precision and r >= min_recall:
            chosen = float(t)
            best = (p, r, t)
            break
    if best is None:
        # fallback: maximize F1
        f1 = 2 * (precision * recall) / np.clip(precision + recall, 1e-9, None)
        idx = int(np.nanargmax(f1))
        if idx < len(thresholds):
            chosen = float(thresholds[idx])
    return chosen


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--features', required=True, help='Path to features parquet (can be .parquet or .parquet.gz)')
    ap.add_argument('--weak-labels', required=False, help='Optional path to weak labels parquet produced by train_pyod.py')
    ap.add_argument('--out-dir', required=False, default=None, help='Output directory (default: models/fraud/dev/YYYYMMDD_HHMM)')
    ap.add_argument('--test-size', type=float, default=0.2)
    ap.add_argument('--seed', type=int, default=42)
    ap.add_argument('--target-precision', type=float, default=0.8)
    ap.add_argument('--min-recall', type=float, default=0.9)
    args = ap.parse_args()

    if args.out_dir is None:
        args.out_dir = os.path.join('models', 'fraud', 'dev', datetime.utcnow().strftime('%Y%m%d_%H%M'))
    os.makedirs(args.out_dir, exist_ok=True)

    df, y = _load_datasets(args.features, args.weak_labels)
    if y is None:
        raise SystemExit('No labels provided in features and no weak labels supplied. Abort.')

    feature_cols = list(df.columns)
    X = df.to_numpy(dtype=np.float32)
    y = y.to_numpy(dtype=np.int32)

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)

    clf, lin = _train_logreg(X_train, y_train)
    # Produces probabilities for the positive class
    val_scores = clf.predict_proba(X_val)[:, 1]

    # Metrics
    roc = float(roc_auc_score(y_val, val_scores))
    # PR AUC
    prec, rec, _ = precision_recall_curve(y_val, val_scores)
    pr_auc = float(auc(rec, prec))

    threshold = _choose_threshold(y_val, val_scores, args.target_precision, args.min_recall)

    # Compute precision/recall at the chosen threshold (for Go-side gating)
    preds = (val_scores >= threshold).astype(int)
    tp = int(((preds == 1) & (y_val == 1)).sum())
    fp = int(((preds == 1) & (y_val == 0)).sum())
    fn = int(((preds == 0) & (y_val == 1)).sum())
    precision_at = float(tp / (tp + fp)) if (tp + fp) > 0 else 0.0
    recall_at = float(tp / (tp + fn)) if (tp + fn) > 0 else 0.0

    # Export model artifact in Go-expected schema
    weights = {feature_cols[i]: float(lin[i]) for i in range(len(feature_cols))}
    bias = float(lin[-1])

    artifact = {
        'version': 'logreg_dev_1',
        'weights': weights,
        'bias': bias,
        'threshold': float(threshold),
        'features': feature_cols,
        'updated_at': datetime.utcnow().isoformat() + 'Z',
        'metrics': {
            'roc_auc': roc,
            'pr_auc': pr_auc,
        }
    }

    out_path = os.path.join(args.out_dir, 'trained_fraud_model.json')
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(artifact, f, indent=2)

    meta = {
        'generated_at': datetime.utcnow().isoformat() + 'Z',
        'features': args.features,
        'weak_labels': args.weak_labels,
        'rows_train': int(X_train.shape[0]),
        'rows_val': int(X_val.shape[0]),
        'roc_auc': roc,
    }
    with open(os.path.join(args.out_dir, 'train_meta.json'), 'w', encoding='utf-8') as f:
        json.dump(meta, f, indent=2)

    print('Wrote model to', out_path)


if __name__ == '__main__':
    main()
