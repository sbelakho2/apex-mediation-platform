# Fraud Model Threshold Selection Playbook

_Date:_ 2025-11-09  
_Owner:_ Fraud ML Team

## Objectives
- Maintain shadow-mode safety by only proposing blocking thresholds that meet business guardrails.
- Provide a deterministic, auditable process that can be automated yet still reviewed by humans.

## Required Inputs
1. Latest evaluation artifact generated by `ML/scripts/evaluate_model.py`.
2. Markdown report under `docs/Internal/ML/Reports/` for qualitative review.
3. Drift snapshot from `models/fraud/monitoring/shadow_monitor_latest.json`.

## Procedure
1. **Load Evaluation Payload**
   - Inspect `models/fraud/workdir/model/evaluation_metrics.json` (or packaged artifact) and identify:
     - `metrics.recommended_threshold`
     - Precision/recall curve arrays
     - Gating decision summary
2. **Verify Target Metrics**
   - Ensure `metrics.precision_at_recall_0_9 ≥ 0.8` _and_ `metrics.recall_at_precision_0_8 ≥ 0.9`.
   - Confirm `metrics.ks_statistic ≥ 0.20` and `metrics.roc_auc ≥ 0.85`.
3. **Confirm Gating Windows**
   - From `train_meta.json` and `evaluation_metrics.json`, verify at least four validation windows.
4. **Drift Check**
   - Examine `shadow_monitor_latest.json`:
     - `drift.population_stability_index ≤ 0.25`
     - `drift.jensen_shannon_divergence ≤ 0.10`
     - `drift.alert` must be `false`.
5. **Set Candidate Threshold**
   - Default to `metrics.recommended_threshold` (computed directly from gate requirements).
   - If a different threshold is preferred, document rationale and expected precision/recall deltas.
6. **Prepare Promotion PR**
   - Update `trained_fraud_model.json` with the proposed threshold under `recommended_threshold`.
   - Include plots/tables from the evaluation markdown in the PR description.
7. **Human Review**
   - Two approvals required: Fraud ML lead and Production safety lead.
   - Reviewers sign off on: metrics, drift health, weak label correlations, and rollout plan.

## Notes
- Threshold computation is deterministic given identical inputs; reruns regenerate identical values due to seeded training (see `train_supervised_logreg.py`).
- Any deviation from the recommended threshold must capture the precision/recall trade-off in the PR.
- If drift alert is `true`, the threshold proposal is automatically rejected; rerun the pipeline after investigating data shifts.
